The goal of the learning process of any learning algorithm is to minimize the expected generalization error, known as \textbf{risk}, i.e. to maximize its accuracy when new inputs are provided. This is achieved via two paths: \textbf{optimization}, which is the process of finding parameters values that minimize the loss function, and \textbf{regularization} which is the process of reducing model capacity avoiding overfitting on the observed data, the \textbf{training set}, and improve generalization performances. To use Mitchell's codification [\cite{Mitchell1997}], we care about some performance measure $P$, that is defined with respect to new unseen observations, the \textbf{test set}.

To measure the generalization performance of the model we compute the loss function, e.g. the MSE, of the model on the new, unseen data. For this purpose, often in practical applications the dataset is divided into training set, used to perform the learning process, and test set, used to measure out-of-sample performances. Sometimes, especially for DL, a third partition is created called \textbf{validation set} and is used to learn the hyperparameters of the model, if not calibrated ex-ante. If not explicitly declared, $N$ will refer to the number of observations in the training set and $N^{test}$ and $N^{val}$ will refer, respectively, to the test and validation set.

To improve generalization performances means to reduce the loss on the test set, $J_{test}$. However, only the training data are used to train the model. One can intuitively minimize the loss, $J_{train}$ on the training set, simply solving for where its gradient is $\nabla_{\theta}J_{train} = 0$, and hope this will lead to improvements on the test set. What we actually care about is the generalization error, computed on the unobserved data. How can we affect the performance on the test set when we can observe only the training set? Statistical theory comes handy in justifying this. We are able to do so thanks to the implicit assumption (\cref{sec:cultures}) that the training and test set are produced by the same data-generating process and each observation is independent of any other. 

Now that we have justified why it is sensible to minimize the loss function on the observed data, how do we actually minimize it? In other words, how do we choose the value for the parameter $\theta$ which minimizes our loss function producing \say{good} estimates $\hat{y}(x)$? 

In the machine learning literature, learning or inference is always cast as an optimization problem. In the next section we will give an overview of the procedures.



\section{Optimization}\label{sec:optimization}
The aim of optimization is to find values of parameter $\theta$ in the parameter space $\Theta$ that minimize the loss function $J(\theta)$, which usually includes a term specifying a performance measure evaluated on the training set and a regularization term to improve the generalization error and avoid overfitting which will be discussed in the next section.

Note that for the purpose of this thesis, we will treat optimization in a supervised learning context, i.e. we are provided with the outcome variable $Y$ as well as the inputs $X$. Furthermore, throughout this section, we will refer to the unregularized optimization case, i.e. the loss function will not contain any regularization term. The development of the regularized cases will be addressed in the next section.

It is common in machine learning applications to use loss functions that decompose as a sum over some per-example loss functions. Therefore, up to a multiplicative constant, the cost function can be seen as an expectation [\cite{Goodfellow-et-al-2016}]. In principle, we would like to minimize the objective function where the expectation is taken over the data-generating distribution, $p_{data}$:
\begin{equation}
    J^\star(\theta) = \mathrm{E}_{(X, Y) \sim p_{data}} L\left( f_\theta(x), y \right) \label{risk}
\end{equation}
that defines the risk, i.e. the expected generalization error, while $f_\theta(x)$ defines the predicted output when the input is $x$.

However, in practise we only have a finite training set. Therefore, we are bound to use a reduced form of $J^\star(\theta)$, namely the \textbf{empirical risk} defined as
\begin{equation}
    J(\theta) = \mathrm{E}_{(X, Y) \sim \hat{p}_{data}} L\left( f_\theta(x), y \right) = \frac{1}{N} \sum_{i=1}^{N} L\left( f_\theta(x_i) ; y_i \right) \label{empirical_risk}
\end{equation}
where $\hat{p}_{data}$ is the empirical distribution on the training set. Therefore, rather than optimizing the risk directly, the empirical risk is minimized with the hope that the risk decreases as well -- a procedure called empirical risk minimization. 

The most important mathematical tool used for optimization is the derivative operator, denoted as $J'(\theta)$ or $dJ/d\theta$. The concept of derivative is crucial for optimization algorithms because it specifies how to scale a small change in the parameters $\theta$ to obtain the corresponding change in the loss function valuation 
$$J(\theta + \eta) \approx J(\theta) + \eta J'(\theta)$$
It tells us that $J(\theta)$ can be reduced by moving $\theta$ in small steps with the opposite sign of the derivative. The type of optimization techniques employing the derivative concept as tool is called \textbf{gradient-based} optimization. In machine learning this is the most used class of methods and will be the object of this section.

How does gradient-based optimization work? Since, as said, often loss functions decompose as a sum over some per-example loss function, the computation of the gradient is performed using the linearity of the derivative operator
\begin{align}\label{gradient}
    g = \nabla_{\theta} J(\theta) & = \nabla_{\theta} \mathrm{E}_{(X, Y) \sim \hat{p}_{data}} L\left( f_\theta(x) ; y \right)\\ \nonumber
    & = \nabla_{\theta} \frac{1}{N} \sum_{i=1}^{N} L\left( f_\theta(x_i) ; y_i \right) \\ \nonumber
    & = \frac{1}{N} \sum_{i=1}^{N} \nabla_{\theta} L\left( f_\theta(x_i) ; y_i \right)  
\end{align}
The most basic example of gradient-based optimization algorithm is \textbf{gradient descent} [\cite{Cauchy1847, Bishop2006}]. It proposes a new parameter value according to the updating rule
\begin{equation}
    \theta^{new} \leftarrow \theta - \lambda g \label{update_rule}
\end{equation}
where $\lambda \in \mathbb{R}$ is an hyperparameter called the \textbf{learning rate}, a positive scalar determining how the gradient affects the proposed value\footnote{Its determination has posed serious challenges to both practitioners and researchers. In subsection (\cref{sec:learning_rate}) we will review ways to set the parameter. Usually for \say{vanilla} gradient descent it is kept fixed during the entire optimization.}.

One issue in applying gradient descent is that, when the data is big, computing the expectation in eq. \eqref{gradient} is computationally very expensive since before an update is proposed, the model must be evaluated on the entire dataset in order to compute the per-example loss. Optimization methods that use the entire training set to compute the gradient are called \textbf{batch} gradient methods. One way to solve the issue is to resort to statistical estimation. Instead of computing the exact gradient, it is approximated by randomly sampling a small number of examples from the training set, computing the per-example loss, and taking the average over them only. This type of optimization methods is called \textbf{minibatch} or \textbf{stochastic} methods. Of course, to compute an unbiased estimate of the expected gradient, minibatches must be selected truly randomly. It can be shown\footnote{\cite{Goodfellow-et-al-2016} pp. 273-274.} that when the stochastic gradient is computed on different observations -- i.e. no observation is resampled -- the approximate expected gradient follows the gradient of the true generalization error, eq. \eqref{risk}. Each time the training set is fully sampled, the procedure restarts again. Each of this iterations is usually called \textbf{epoch}.

The basic example of a stochastic optimization method is \textbf{stochastic gradient descent}. The update rule for the parameters is the same as in eq. \eqref{update_rule} but instead of an exact $g$, an approximate $\hat{g}$ is used. The learning rate usually is not held fixed, but it is gradually decreased over time. This is necessary since the estimation of $g$ introduces noise, via the random sampling, that does not vanish even when a minimum is reached. Adaptive learning rate methods are discussed below. On the contrary, batch optimization methods, such as gradient descent, can hold $\lambda$ fixed over time\footnote{For more details on how to tackle the issue, the reader is redirected to \cite{Goodfellow-et-al-2016} ch. 8.},

Another example of stochastic optimization is the \textbf{momentum} method. This algorithm adds a variable $v$, called \textit{velocity}, that stores value of an exponentially decaying moving average of the past gradient and suggests the algorithm to move in the same direction. A memory parameter $\alpha \in [0,1)$ determines how much past information to store. The update rule becomes, denoting the size of the minibatch with $m$, 
\begin{align*}
    v^{new} & \leftarrow \alpha v - \lambda \nabla_{\theta} \left(\frac{1}{m} \sum_{i=1}^{m} L\left( f_\theta(x_i) ; y_i \right)\right), \quad m < N\\
    \theta^{new} & \leftarrow \theta + v^{new}
\end{align*}
Given $\lambda$ -- that should be modeled to vary in some ways -- the size of the step now depends on how large and aligned the sequence of gradients is.

A slightly modified version of the momentum algorithm is the \textbf{Nesterov momentum}. The gradient is evaluated after the velocity is applied. The update rule in this case is
\begin{align*}
    v^{new} & \leftarrow \alpha v - \lambda \nabla_{\theta} \left(\frac{1}{m} \sum_{i=1}^{m} L\left( f_{\theta+\alpha v}(x_i) ; y_i \right)\right)\\
    \theta^{new} & \leftarrow \theta + v^{new}
\end{align*}

It is important to note that given the common non-convexity of loss functions in deep learning, an important factor impacting the convergence of optimization algorithms is parameter initialization, i.e. the starting point of the path towards the minimum. This is the reason why, sometimes optimization is brought about using a completely different approach: training a simpler model and use the trained parameters as initial values of the more complex model. This technique is called \textit{supervised pretraining} [\cite{Goodfellow-et-al-2016}]. 




\subsection{The learning rate problem} \label{sec:learning_rate}
The convergence speed of SGD depends on the variance of the gradient estimates which, in turn, depends on the size of the mini-batch: by the law of large numbers, increasing the mini-batch size reduces the stochastic gradient noise. Smaller gradient noise allows for larger learning rates and leads to faster convergence. For \say{vanilla} gradient descent where we use the entire dataset, the learning rate can be relatively bigger with respect to using stochastic gradient descent.  Therefore, there is a trade-off between the computational overhead associated with processing a mini-batch of a bigger size, and the computational cost of performing more gradient steps due to the smaller learning rate. 

To accelerate the learning procedure, one can either optimally adapt the mini-batch size for a given learning rate, or optimally adjust the learning rate to a fixed mini-batch size. The latter approach is the preferred in the machine learning literature. 

How does the learning parameter need to be chosen? This question has multiple answers, each corresponding to a different algorithm. Here we briefly review the most important that will be useful in actual applications. The basic idea is that in each iteration, the empirical gradient variance can guide the adaptation of the learning rate which is inversely proportional to the gradient noise. Popular optimization methods that make use of this idea include

\paragraph{Delta-bar-delta.} The delta-bar-delta [\cite{Jacobs1988}] algorithm was one of the first heuristic method to adapt the learning rate of batch optimization methods. The rule is simple: if the partial derivative of the loss with respect to a given model parameter remains the same sign, then the learning rate should increase, otherwise the learning rate should decrease. 

\paragraph{AdaGrad.} A similar heuristic approach is AdaGrad [\cite{Duchi2011}] that scales the individual model parameters inversely proportional to the square root of the sum of all the historical squared values of the gradient. 

\paragraph{RMSProp.} A modification of AdaGrad is provided by the RMSProp algorithm [\cite{Hinton2012}]. It changes the gradient accumulation into an exponentially weighted moving average commanded by an hyperparameter $\rho$ determining the algorithm's memory. 

\paragraph{Adam.} The Adam algorithm combines RMSProp to learn the learning rate with the momentum method [\cite{Adam2014}].


\section{Regularization} 
As said, the aim of a learning algorithm is twofold: minimizing the training error and the the gap between training and test error. In other words, avoiding
\begin{itemize}
    \item \textbf{Underfitting}: high error rate on the training set
    \item \textbf{Overfitting}: large gap between training error and generalization error
\end{itemize}
We can control whether a model is more likely to overfit or underfit by altering its \textbf{capacity}, that is its ability to fit certain variety of functions -- e.g. linear regression as a lower capacity than polynomial regression -- and one way to control the capacity of a learning algorithm is by choosing its \textbf{hypothesis space}, the set of functions that the learning algorithms is able to approximate, appropriately.

Regularization refers to strategies designed to reduce the generalization error, often paying the price of higher training error. They can all be seen as a way to reduce the capacity of the model. In general such strategies can be categorized into two classes: 
\begin{enumerate}
    \item Strategies that put extra constraints on a learning model  (e.g. adding restrictions on parameters' values or on the activation functions)
    \item Strategies that add extra terms in the objective function
\end{enumerate}
These constraints and penalties can be generally interpreted as encoding some specific kind of prior knowledge or expressing preferences for simpler model classes in order to improve generalization reducing the threat of overfitting.

In the context of deep learning, most regularization strategies are based on regularizing estimators that have the effect of increasing bias while reducing variance. As usual, the bias of an estimator is defined as $\text{Bias}(\hat{\theta}) = \mathrm{E}_{\hat{p}_{data}}(\hat{\theta} - \theta)$ which defines the expected error of the estimator, while the variance $\mathrm{V}(\hat{\theta})$ defines the error in the estimation stemming from the sensitivity to small fluctuations in the sample observed. High bias can cause an algorithm to mistake the mapping between features and target outputs (underfitting), while high variance can cause an algorithm to model the random noise in the training data rather than the intended outputs (overfitting). Therefore, an effective regularizer is one that is able to find the right trade-off, reducing variance significantly while not overly increasing the bias. 

Via regularization we want to restrict at minimum the capacity of the model to include the data-generating process ruling out other possible candidate data-generating processes in the scope of the model capacity (reducing the variance). This conditions of the model capacity should not be too restrictive: the risk is to rule out of the scope of the model capacity the true data-generating process (bias). Unfortunately, we almost never have access to the true data-generating process and thus we can never be completely sure that the class of models estimated includes the true generating process. 




\subsection{Parameter Norm Penalties} \label{subsec:parameter_norm}
A commonly used example of regularization approach belonging to the second class of strategies are the \textbf{parameter norm penalties}. These approach entails limiting the capacity of the model by adding a penalty in the loss function $J(\theta)$ proportional to the \textit{parameter norm} $\Omega(\theta)$. Note that usually for neural networks the norm is computed only on the weights $W$, while the biases $b$ are left unregularized. The regularized cost function can be written as
\begin{equation}
    \tilde{J}(\theta) = J(\theta) + \alpha \Omega(\theta)
\end{equation}
where $\alpha \in [0, \infty)$ weights how the norm penalty affects the regularized objective function relative to the standard loss function. In neural networks, usually, the penalties are separated per layer and a different $\alpha$ is used for each layer. In practical applications, due to computational costs, the definition of $\Omega$ used is often the same for each layer. Different definitions of $\Omega$ result in different solutions being preferred. The two widely known definitions used are:

\paragraph{$L^2$-norm.} In this case we have $\Omega(\theta) = \frac{1}{2}\Vert \theta \Vert_{2}^{2}$, commonly known as \textbf{weight decay}. Therefore the regularized loss becomes
\begin{align*}
    \tilde{J}(\theta) &= \frac{\alpha}{2} \theta^\top \theta + J(\theta)\\
    \nabla_{\theta} \tilde{J}(\theta) &= \alpha \theta + \nabla_{\theta} J(\theta)\\
    \intertext{defining the update rule}
    \theta^{new} &\leftarrow \theta - \lambda \big( \alpha\theta + \nabla_{\theta}J(\theta) \big)
\end{align*}
The effect of the regularization term is to shrink $\theta$ by a constant factor on each step, just before performing the usual gradient update.

\paragraph{$L^1$-norm.} In this case we have $\Omega(\theta) = \Vert \theta \Vert_{1} = \sum_{i}|\theta_i|$, obtaining
\begin{align*}
    \tilde{J}(\theta) &= \alpha \Vert \theta \Vert_{1} + J(\theta)\\
    \nabla_{\theta} \tilde{J}(\theta) &= \alpha sign(\theta) + \nabla_{\theta} J(\theta)
\end{align*}
In this case, the gradient does not scale linearly as with the $L^2$-norm definition, but it is a constant factor with a sign equal to the sign of $\theta$. The gradient so defined has no longer an algebraic solution and must be approximated. It can be shown [\cite{Goodfellow-et-al-2016}] that $L^1$ regularization results in a solution that is more sparse, i.e. parameters will tend to have optimal value of zero. This can be interpreted as a form of variable selection mechanism. The LASSO is an example of this mechanism employed in linear models training. 

The parameter norm penalties regularization can also be interpreted as MAP Bayesian inference: e.g. $L^2$ regularization is equivalent to MAP Bayesian inference with a Gaussian prior on $\theta$.




\subsection{Early Stopping}\label{subsec:early_stopping}
Often when training neural network with sufficient representational capacity, a signal that the model is overfitting is that training error decreases steadily over time, but validation set error begins to rise again. The validation set is used to evaluate a given model, like the test set, frequently during training. It corresponds to a small portion of the data that are mainly used to fine-tune the model hyperparameters. Hence the model \say{occasionally} sees the validation data, but never learns from them.

In principle, we can expect that minimizing the validation set error would also lead to better test error. Therefore, rather than stopping the optimization algorithm when the training error is minimum, it is possible to stop it earlier, when the validation error is the least, i.e. we use the parameters values in correspondence of the least validation error, rather than the latest values returned by the optimization algorithm. 

This strategy is known as early stopping. In practise, the optimization algorithm is instructed to stop when no parameters have improved over the best recorded validation error for some pre-specified number of iterations. In deep learning, this is the most commonly used regularization strategy. In this way, we control how many times the optimization algorithm can run to fit the training set, limiting the threat of overfitting. This strategy does not require any change in the underlying procedure, loss function, or parameter set.



\subsection{Sparse Representations}
Another approach to regularize neural networks is to impose a penalty on the activation functions of the units by introducing sparseness. Essentially, the model is brought to prefer configurations in which more hidden units are set to zero; this reduces the capacity of the model. Indirectly, it is like imposing a complex penalization on the model parameters, As mentioned in subsection (\cref{subsec:parameter_norm}), also $L^1$-penalization introduces sparseness in the parameters values. 

Sparseness is induced in the representations, i.e. components of the hidden layer $h_l$ are set to zero, not in the components of the parameters $W_l$ or $b_l$. In practice, this regularization is applied to the model via adding a parameter norm penalty over the dimension of the hidden units in the loss function
\begin{equation*}
    \tilde{J}(\theta) = J(\theta) + \alpha \Omega(h), \quad \alpha \in [0,\infty)
\end{equation*}
Note how the argument of $\Omega$ is now the $h$ vector and not $\theta$.



\subsection{Bagging and Ensemble Methods}\label{subsec:bagging}
The underlying idea is to reduce generalization error combining different models. In practise it is performed by training different models separately and for each input, $X_i$, each of these models collect the proposal $f_i(X_i)$. Techniques using this strategy, known as \textit{model averaging}, go under the label of \textbf{ensemble methods}.

On average the ensemble performs at least as well as any of its members. In case the errors made by the individual models are independent, the ensemble performs significantly better than them.

A particular example is the \textbf{bagging} method, short for \say{bootstrap aggregating}. This approach allows the very same model to be reused, instead of fitting different models. In practise $k$ dataset with the same number of observations of the training set are constructed by sampling with replacement from the training set. Model $i$ is trained on dataset $i \in \{1, \dots, k\}$. Given that the same model is used, the difference among the models in the ensemble is due to the different $k$ \say{artificial} dataset on which the model is trained.



\subsection{Dropout}
\textbf{Dropout} is a type of ensemble method, but for the importance it will have in this thesis -- and in practise -- it deserves a dedicated subsection. 

Bagging entails training the same model multiple times on different training sets. For large neural networks the computational cost soon skyrockets. Dropout provides a computationally inexpensive approximation to train and evaluate a \textit{bagged} ensemble. In particular, dropout trains the ensemble consisting of all subnetworks that can be formed by removing non-output units from the underlying base network. The number of submodels, thus, grows exponentially in the number of units. Switching off a unit -- in the input layer or hidden layers -- is performed by multiplying it by zero. Dropout applied to input units serves as a form of variable selection [\cite{Hinton2006, Srivastava2014}], interpretable as Bayesian ridge regression [\cite{Polson2017}]. When applied to hidden layers it regularizes the choice of the number of hidden units in a layer. Once a variable from a layer is dropped, all terms above it in the network also disappear.

In practise, dropout removes units in the input layer and/or in the hidden layers randomly with a given
probability $p$, different for each layer. Training with dropout is performed using stochastic optimization methods like SGD. Each time a minibatch is created, for each layer, a binary vector $\zeta_l$ is randomly sampled, where each component is distributed as a $\mathrm{Bernoulli}(p_l)$. The number of components in the binary vector matches the number of units in the layer. This vector is usually called \textbf{mask}. The components of $\zeta$ are sampled independently of each other. The probabilities $p_l$ are hyperparameters fixed before training begins. Therefore the original input layer becomes $\tilde{x} = x \odot \zeta_0$, where $\odot$ represents the Hadamard (or element-wise) product. Similarly, each hidden layer becomes $\tilde{h}_l = \zeta_l \odot h_l, \; l = 1, \dots, L$. The same values of the binary vector are used during optimization, namely when performing back-propagation. 

Let $J(\theta, \zeta)$ define the cost of the model defined by parameters $\theta$ and the mask $\zeta$.
Marginalizing over the randomness, the objective becomes 
$$\mathrm{E}_{\zeta \sim \mathrm{Ber}(p)} J(\theta, \zeta)$$
The expectation contains exponentially many terms, but an unbiased estimate of its gradient can be obtained by sampling and using only certain values of $\{\zeta_0, \dots, \zeta_L\}$. This differentiate dropout from bagging: in the latter case, each model is trained until convergence on its respective training set; in the former case, most models are not explicitly trained at all, but only a tiny fraction of the possible subnetworks are trained for a single step, and the parameter sharing, across models with the same active units, causes the remaining subnetworks to arrive at good settings of the parameters. 

To conclude, we have to highlight that optimization, per se, does not provide an uncertainty measure. This is solve by taking a Bayesian approach.







\section{Bayesian Neural Networks}
Bayesian Neural Networks (BNN) were first suggested in the '90s and studied extensively since then [\cite{MacKay1992, Neal1996}], they are now living a period of huge fame. They offer a probabilistic interpretation of neural networks models by inferring distributions over the models' weights (usually biases are left unbounded). The model offers robustness to overfitting, uncertainty estimates, and can easily learn from small datasets. As in any Bayesian inference problem, BNN place a prior distribution over a neural networkâ€™s weights, which induces a distribution over a parametric set of functions. 

Given training inputs $x$ and their corresponding outputs $y$, in Bayesian (parametric) regression we would like to find the parameters $\theta$ of the function $f_\theta$ such that $y = f_\theta(x)$, i.e. those values of the parameters that are likely to have generated the outputs. Following the Bayesian approach we would put some prior distribution over the space of parameters, $p(\theta)$. This distribution represents our prior belief as to which parameters are likely to have generated our data before we observe any data points. Once some data are observed, this distribution will be transformed to capture the more likely and less likely parameters given the observed data points. To perform inference, we further need to define a likelihood distribution, i.e. the model for our data, $p(y|x, \theta)$ -- the probabilistic model by which the inputs generate the outputs given some parameter setting $\theta$. 

We then look for the posterior distribution over the space of parameters by invoking the Bayes' theorem:
\begin{equation}
    p(\theta|x,y) = \frac{p(y|x, \theta) \; p(\theta)}{p(x)} \label{eq:posterior}
\end{equation}
where $p(x) = \int p(y|x, \theta) p(\theta)\; d\theta$. Once we compute this posterior, we can predict an output for a new input point $x^*$ by integrating out the parameters
$$p(y^*|x^*) = \int p(y^*|x^*, \theta) \; p(\theta|x,y) \; d\theta$$
Performing this integration is also referred to as \textit{marginalising} the likelihood over $\theta$, which explains the alternative name for the model evidence: \textit{marginal likelihood}. Marginalisation can be done analytically for simple models such as Bayesian linear regression. In such models the prior is conjugate to the likelihood, and the integral can be solved with known tools of calculus. Marginalisation is the one of core parts in Bayesian modelling, and ideally we would want to marginalise over all uncertain quantities -- i.e. averaging with respect to all possible model parameter values $\theta$, each weighted by its plausibility $p(\theta)$. But with more interesting models (even basis function regression when the basis functions are not fixed) this marginalisation cannot be done analytically. In such cases an approximation is needed.




\section{Variational Inference}\label{sec:VI}
Modern research in BNN often relies on either \textit{variational inference}, or \textit{sampling based techniques} like MCMC. Each approach has its merits and its limitations. MCMC methods tend to be more computationally intensive than variational inference but they also provide guarantees of producing (asymptotically) exact samples from the target density [\cite{RobertCasella2005}]. Variational inference does not enjoy such guarantees -- it can only find a density close to the target -- but tends to be faster than MCMC. Because it rests on optimization, variational inference easily takes advantage of methods like stochastic optimization and distributed optimization. Also some MCMC methods can also exploit these innovations [\cite{Welling2011}], but they are still not well established as variational inference. 

To provide an overview of the method, let's setup the general problem. Consider a joint density of latent variables $z = (z_1,\dots,z_m)$ and observations $x = (x_1,\dots, x_n)$. For example, in the case of a Gaussian distribution with unknown mean and variance, $z = (\mu, \sigma^2)$. In the case of a neural network, $z$ is equal to all weights in the network. We can write 
$$p(x,z) = p(z)\;p(x|z)$$
A typical Bayesian model draws the latent variables from a prior density $p(z)$ and then relates them to the observations through the likelihood $p(x|z)$. Inference, thus, amounts to conditioning on data and computing the posterior $p(z | x)$. In complex Bayesian models, like neural nets, this computation often requires approximate inference. 

As discussed above, the dominant paradigm for approximate inference has been MCMC. In MCMC, we first construct an ergodic Markov chain on $z$
whose stationary distribution is the posterior $p(z|x)$. Then, we sample from the chain to collect samples from the stationary distribution. Finally, we approximate the posterior with an empirical estimate constructed from a subset of the collected samples. 

The key characteristic of variational inference is that it transforms inference problems into optimization. Thus, variational inference is suited to large data sets and scenarios where we want to quickly explore many models; MCMC is suited to smaller data sets and scenarios where we are willing to pay a heavier computational cost for more precise samples. 



\subsection{Preliminaries}
First, we assume a family of approximate densities $\mathcal{Q}$, that is a set of densities over the space of the latent variables. Then, we try to find the member of that family that minimizes the Kullback-Leibler
(KL) [\cite{KL1951}] divergence from the exact posterior,
\begin{equation}
    q^\star(z) = \underset{q(z)\in\mathcal{Q}}{\arg \min} \; \mathrm{KL}[q(z)\Vert p(z|x)] 
    \label{eq:VI}
\end{equation}
Finally, we approximate the posterior with the optimized member of the family $q^*(z)$. In other words, the goal in VI is to approximate the posterior by a simpler distribution. To this end, one minimizes the KL  divergence between the approximating distribution and the posterior. One of the key ideas behind variational inference is to choose $\mathcal{Q}$ to be flexible enough to capture a density close to $p(z | x)$, but simple enough for efficient optimization.

Unfortunately, the objective, \eqref{eq:VI}, is not computable because it requires computing the evidence $\log p(x)$. In fact $p(z|x) = p(x,z)/p(x)$. To appreciate this recall that the KL divergence is
\begin{align}
    \mathrm{KL}[q(z)\Vert p(z|x)] &= \mathrm{E}[\log q(z)] - \mathrm{E}[\log p(z|x)] \nonumber\\
    \intertext{where all expectations are taken with respect to $q(z)$. Expanding the conditional,}
     &= \mathrm{E}[\log q(z)] - \mathrm{E}[\log p(z, x)] + \log p(x) \label{eq:KL}
\end{align}
and the dependence on the log-evidence is revealed.

Since we cannot compute the KL, we optimize an alternative objective that is equivalent to the KL up to an \textit{added} constant,
$$\mathrm{ELBO}(q) = \mathrm{E}[\log p(z, x)] - \mathrm{E}[\log q(z)]$$
This function is called the \textbf{evidence lower bound} (ELBO). The ELBO is the negative KL divergence in eq. \eqref{eq:KL} plus $\log p(x)$, which is a constant with respect to $q(z)$. Maximizing the ELBO is equivalent to minimizing the KL divergence in eq. \eqref{eq:KL}. To make explicit the dependence of the ELBO on the conditional log-likelihood of the data given the latents, we can rewrite $p(z,x) = p(x|z)p(z)$, thus
\begin{align}
    \mathrm{ELBO}(q) &= \mathrm{E}[\log p(x|z)] + \mathrm{E}[\log p(z)] - \mathrm{E}[\log q(z)] \nonumber\\
                &= \mathrm{E}[\log p(x|z)] - \mathrm{KL} [q(z)\Vert p(z)] \label{eq:ELBO}
\end{align}
The first term is the expected likelihood: it encourages densities that place their mass on configurations of the latent variables that explain the observed data. The second term is the negative divergence between the variational density and the prior: it encourages densities close to the prior. Thus the variational objective mirrors the usual balance between likelihood and prior. The KL term can also be interpreted as a form of regularization that favours simpler models. 

Furthermore, rewriting eq. \eqref{eq:KL} as 
$$\log p(x) = \mathrm{KL} [q(z)\Vert p(z | x)] + \mathrm{ELBO}$$
and considering that $\mathrm{KL}(\cdot) \geq 0$, we can assess that the ELBO is a lower-bound for the marginal log-evidence, $\log p(x) \geq \mathrm{ELBO}(q)$ for any $q(z)$. This was originally shown via the Jensen's inequality in [\cite{Jordan1999}]. To sum up, this approach circumvents computing intractable normalization constants. 



\subsection{Classical Variational Inference}
We slightly modify the simplified notation of the previous section making explicit the parametrization of both $p_\theta(\cdot)$ and $q_\phi(\cdot)$. To simplify the exposition, we assume that $\theta$ and $\phi$ are random variables that call for a Bayesian treatment, they are estimated by maximum likelihood. This assumption will be relaxed in section (\cref{sec:reparametrization}). 
%Furthermore, we consider $z$ to be a \say{true} latent variable: it will not represent the parameters of the model.

As in the previous section, the model has observations $x$ and latent random variables $z$ whose distributions are both parametrized by $\theta$. It has a joint probability density of the form
$$p_\theta(x,z)=p_\theta(x|z)\;p_\theta(z)$$
This kind of model is also called \textit{latent variable model}. It is a probability distribution over two sets of variables, $x$ and $z$, where the former are observed and the latter are never observed. 

Our goal is to compute the maximum likelihood estimate of the parameters
\begin{align*}
    \theta_{MLE} &= \underset{\theta}{\arg\max} \; \log p_\theta(x)\\
    \intertext{as well as the posterior over the latent variables}
    p_{\theta_{MLE}}(z|x) &= \frac{p_{\theta_{MLE}}(x,z)}{\int p_{\theta_{MLE}}(x,z)\; dz}
\end{align*}
Variational inference offers a scheme for finding $\theta_{MLE}$ and computing an approximation to the posterior $p_{\theta_{MLE}}(z|x)$. As said in the previous section, even with $\theta$ known, the integral would be intractable. Hence, we apply variational inference introducing the \textit{variational distribution} $q_\phi(z)$ parametrized by the variational parameters $\phi$. The members of the family of approximate densities $\mathcal{Q}$ on $z$, is, thus, indexed by $\phi$. Equation \eqref{eq:VI} can be rewritten as
\begin{equation}
    \phi^* = \underset{\phi}{\arg\min} \; \mathrm{KL}[q_\phi(z) \Vert p_\theta(z|x)]
\end{equation}
Following the same reasoning of eq. \eqref{eq:ELBO}, we can rewrite the ELBO as a function of the variational parameters
\begin{align}
    \mathrm{ELBO}(\phi) &= \mathrm{E}[\log p_\theta(x,z)] - \mathrm{E}[\log q_\phi(z)]\nonumber\\
                &= \mathrm{E}[\log p_\theta(x|z)] - \mathrm{KL} [q_\phi(z)\Vert p_\theta(z)] \label{eq:ELBOphi}
\end{align}
where all the expectations are with respect to $q_\phi(\cdot)$.
Therefore, we can now state that VI turns Bayesian inference into an optimization problem over variational parameters. To complete the specification of the optimization problem, we have to define the family of variational distributions $\mathcal{Q}$. 

In traditional VI, computing the ELBO amounts to analytically solving the expectations over $q$. This restricts the class of tractable models and thus restrict our choice regarding the family $\mathcal{Q}$. The \textbf{mean-field variational family} is a class of distributions whose members are of the form
$$q_\phi(z) = \prod_i q_{\phi_i}(z_i)$$
The latent variables in $z$ are mutually independent, each one is described by a different factor in the variational density, and each factor is governed by its own variational parameter. In principle, each of the factors can take on any parametric form appropriate to the corresponding random variable. A fully factorized variational distribution allows one to optimize the ELBO via simple iterative updates. A common optimization algorithm for this case is the \textit{coodinate ascent variational inference} (CAVI) [\cite{VIstat2016}].

The mean-field family, though, is limited in multiple ways when it comes to modern applications. The three major drawbacks are the following:
\begin{itemize}
    \item It is expressive because it can capture any marginal density of the latent variables, however, it cannot capture correlations among them
    \item It is not scalable to big datasets
    \item To use CAVI we have to be able to express the ELBO analytically
\end{itemize}
These three limitations are huge constraints especially in a deep learning context where the ELBO cannot be evaluated analytically. All these limitations will be addressed in chapter 3, where we will show that VI can be extended to accommodate all the needs of deep learning applications. 






\section{Bayesian Deep Learning}
With Bayesian deep learning we refer to the application of Bayesian methods to deep neural networks. Most of the research in this field is focused on the statistical interpretation of regularization or optimization procedures customarily used in training neural networks. Bayesian estimation is usually brought about via variational inference. The aim is to demonstrate that the cost functions minimized by applying variational methods actually coincide, or can be interpreted, as the cost functions minimized using practical regularization techniques.

For example, [\cite{Gal2015, Gal2016}] proved that training deep neural nets with dropout corresponds, under mild assumptions on the shape of the variational distribution, to approximate inference in deep neural nets. Dropout, randomly eliminating some nodes of the network, inject noise into the feature space. In these two seminal works, the authors proved that this noise can be transformed into noise on the parameter space. This procedure is referred to as \textbf{MC dropout}.

Furthermore, [\cite{Salas2018}] provide a probabilistic interpretation of adaptive subgradient methods such as AdaGrad and Adam. They set out a framework in which it is possible to perform inference on the posterior distribution of the weights of a deep neural network. They start by applying a second-order expansion to the cost function around the current iterate of the stochastic optimization, that is around the value of the parameters at the current iteration of the stochastic gradient descent process. By imposing an improper prior on the network weights, they recover an approximate posterior distribution for the network weights.

In the next chapter we will discuss Deep Bayesian Learning which refers to the application of deep learning to Bayesian methods. In particular, we will discuss how using deep neural networks to parameterize distributions can improve inference.





\vspace{4cm}